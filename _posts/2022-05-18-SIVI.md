---
toc: true
layout: post
description: An introduction to Semi-Implicity Variational Inference
categories: [markdown]
title: SIVI
use_math: true
---

# SIVI (Semi Implicity Variational Inference)

SIVI 是貝葉思推斷的進階觀念，在討論 SIVI 之前，我們先來談談何謂變分推斷、模型推斷、以及 SIVI 在解決什麼問題。

本文框架皆是在監督式學習以及統計的角度下看待神經網路和機器學習

## Model Inference

模型推斷基本上是機器學習中最常用的技巧之一。我們手中的資料通常都是樣本的特徵 $x_i = (x_{i1}, \cdots, x_{ip})$ 加上樣本對應的標籤 $y_i$，這樣成隊出現的資料有一個概率 $p(x,y)$，也就是我們蒐集到這筆資料的概率，或者說這筆資料是從一個機率分配 $p$ 產生的，可以是非常複雜的機率分配，也可以是非常簡單的

我們希望用蒐集到的資料來建立模型，最後可以用在測試集上預測，也就是窮舉所有的 $y$，
找一個 $y$ 使得 $y=\arg\max_{y'} p(y'\midx_{new})$，這樣的問題可以透過貝葉思解決，
也就是透過 $p(x,y)=p(x\midy)p(y)$ 來估計 $p(y\midx)$

$$
p(y\midx) = \frac{p(x\midy)p(y)}{\int p(x\midy)p(y)dy} 
$$

$p(x\midy)$ 可以是任何一個可解析形式，例如在樸素貝葉思中假設 $p(x\midy) \sim N(\mu_y,\sigma^2_y)$。
由於先驗分配 $p(x)$ 積分運算太過複雜，上式中的解通常沒有一個可解析的形式。
變分推斷就是透過建立一個變分分配 $q(x)$ 來估計 $p(y\midx)$

## 變分推斷

在變分推斷中，我們會假設觀測值 $x$ 是被一個潛在變量 $z$ 影響，這到底是什麼意思呢? 我常常會舉這樣的一個例子，假設你今天看到小明出門帶雨傘，則小明帶雨傘是你的觀測值，那麼小明為什麼會帶雨傘呢? 你不知道，因為這是一個隱藏的變量，有可能是因為天氣預報，也有可能是因為他媽媽叫他帶的，不管是哪種都會影響小明做出要不要帶雨傘出門的決定，這就是隱變量，或者潛變量

我們前面提到，變分推斷希望找到一個變分分配 $q(z)$ 來代表 $p(z\mid x)$，當然，我們希望這兩個分配的差別越小越好，一種計算這種分配之間的抽象距離是透過 KL 散度

- $\text{KL(q(z)\mid\midp(z\midx))} = \log p(z\midx) + \mathbb{E}_{q(z)}[\log q(z)-\log p(x, z)]$
- $\text{ELBO}:$ - $\mathbb{E}_{q(z)}[\log q(z)-\log p(x, z)]$
- $\text{Target}:$ Maximize $\text{ELBO}$

我們的任務是盡量讓估計的分配與真實更為接近，也就是 $\min_{q(z)}\text{KL(q(z)\mid \mid p(z\midx))}$，我們注意到在 KL 散度的第一項與 $q$ 無關，所以我們需要做的只有最小化後面第二項 (-ELBO)，也等同於最大化 $\text{ELBO}$。

那麼問題來了，這個 $q(z)$ 具體怎麼算呢? 有一種方法是 MFIV，他假設隱藏變量之間是沒有關係的，也就是

$$
q(z) = \prod q_i(z_i)
$$

不過這種假設跟樸素貝葉思一樣，太過天真了!! 因為現實蒐集到的資料大多是有相關的。

## SIVI 半隱變分推斷

先講講何謂隱分配，如果我們不知道一個分配具體的形式長成什麼樣子，可是我們卻可以從中做取樣的動作，則這個分配稱為隱分配。

這樣講可能不是特別具體，設想神經網路的生成器，我們不知道生成器中做了什麼操作，但是我們可以給定一個特定的 noise 生成出我們想要的結果，也就是說我們從一個沒有具體形式的黑盒子中抽樣得到一筆新的 data，所以我們可以將神經網路看成是一種隱分配。

### 半隱分配

SIVI 中的重點是定義了半隱分配，如果一個隱分配的形式可以寫成以下形似，則稱為半隱分配

$$
q(z)=\int q_\xi(z\mid \psi)q_\phi(\psi)d\psi
$$

上式中的 $\psi, \phi$ 為兩個不同分配的參數，其中

- $q_\xi(z\mid\psi)$ 為可解析形式，也就是我們知道具體分配長什麼樣子
- $q_\phi(\psi)$ 可為不可解析形式，也就是我們不知道具體分配長什麼樣子

我們可以看到積分中一半為可解析，一半為不可解析，所以稱為''半隱''。事實上，任一個隱分配 $q(z)$ 都可以被半隱分配逼近的無限好

$$
q_{\phi}(z) \approx \int N(z\mid\mu, \sigma)q_{\phi}(\mu)d\mu, \ \sigma^2 \to 0
$$

舉例來說，我們可以設定 $q_\xi(z\mid \psi)\sim N(\mu_\psi,\Sigma_\psi)$，其中 $\psi=T_\phi(\epsilon)$ 有 $\epsilon\sim N(0,1)$

### 抽樣問題

我們的目的是要從 $q(z)$ 中抽樣，所以我們只要能有效地從隱分配 $q_{\phi}(\psi)$ 中抽樣即可 (因為 $q_\xi(z\mid\psi)$ 為可解析形式)

SIVI 估計 $q_{\phi}(z)$ 的核心概念是透過蒙地卡羅的方法

$$
\begin{aligned}
q(z)&=\int q_\xi(z\mid\psi)q_\phi(\psi)d\psi \\
&\approx \frac{1}{K}\sum_{k=1}^Kq_\phi(z\mid\psi^k) , \ \ \psi^k \sim q_{\phi}(\psi)
\end{aligned}
$$

舉例來說 $q_{\phi}(\psi)$ 的抽樣可以是神經網路 $T_\phi(\epsilon)$ 的輸出，其中 $\epsilon$ 是 noise，$\phi$ 是網路參數

### Optimization

接下來我們來討論如何找到 $q(z)$，回顧一下我們的任務是最大化 $\text{ELBO}$，這部分的內容較多數學推導，有興趣的同學再去閱讀作者的原論文 [SIVI](https://arxiv.org/pdf/1805.11183.pdf)，這裡我只擷取重點，對數學不感興趣的請直接跳到應用

根據 KL 散度的凹性 (計算高維分配的 KL 散度比計算邊際分配的 KL 散度在平均困難)，我們有 ELBO 的下界為 $\underline L(q(z\mid\psi),q_\phi(\psi))=-\mathbb{E}_{\psi\sim q_\phi(\psi)}\text{KL}(q(z\mid \psi)\mid \mid p(z\mid x))+\log p(x)$

- Convexity: $\text{KL}(\mathbb{E_\psi q(z)\mid\midp(z\midx)}) \leq \mathbb{E_\psi}\text{KL}(q(z)\mid \mid p(z\midx))$
- ELBO 下界: $\underline L(q(z\mid\psi),q_\phi(\psi))=-\mathbb{E}_{\psi\sim q_\phi(\psi)}\text{KL}(q(z\mid\psi)\mid\midp(z\midx))+\log p(x)$

但是我們注意到，如果直接優化 $\underline L$，會讓該式的解掉進 $\phi^'$，這樣並不是我們想要的，所以我們會在原本的目標函數中加上正則項來避免掉進局部極小值

- $\phi^'=\arg\max_\phi-\mathbb{E}_{z\sim q(z\mid\psi)}\log\frac{q(z\mid\psi)}{p(z,x)}$

### Regularization

我們在原本的 $\underline L$ 中加入了一個正則項 $B_K$ 來避免 SIVI 退化到一班的 VI

- $B_k=\mathbb{E}_{\psi,\psi^{(1)},\cdots,\psi^{(k)}\sim q_\phi(\psi)}\text{KL}(q(z\mid \psi)\mid\mid\tilde h_K(z))$
- $\tilde h_K(z)=\frac{q(z\mid\psi)+\sum q(z\mid\psi^{(k)})}{K+1}$
- 注意到 $B_K \geq 0$ 且 $B_K = 0$ 僅且僅當 $K=0$ 或 $q_\phi(\psi)=\delta_{\phi^'}(\psi^')$

所以我們的目標函數變成 $\underline L + B_K$，隨著我們抽樣的越多 ($K$ 越大)，$\underline L + B_K$ 會漸進收斂到 ELBO

$$
\begin{aligned}
\underline L + B_K = &\mathbb{E}_{\psi,\psi^{(1)},\cdots,\psi^{(k)}\sim q_\phi(\psi)}\text{KL}(q(z\mid\psi)\mid \mid \tilde h_K(z))\\ -&\mathbb{E}_{\psi\sim q_\phi(\psi)}\text{KL}(q(z\mid\psi)\mid \mid p(z\midx))+\log p(x)
\end{aligned}
$$

### Algorithm

![](https://i.imgur.com/6q3j6o4.jpg)

## 應用

SIVI 一個最大的應用是在變分自動編碼器上 (VAE)，接下來我們來談談什麼是 VAE，他又與我們今天的主題什麼關係

### VAE

我們知道任何一張圖片或者一段文字都可以向量化來表示，換句話說，影像或者文字都來自於一個分配 $p(x)$，這些影像或者文字可能是被某種我們不知道的元素影響 (潛變量 $z$)，VAE 用 $q_{\phi}(z\mid x)$ 來逼近 $p(x)$ ，試圖在潛在空間找到潛變量和輸入的關係，至於分配 $q(z\mid \mu(x,\phi),\sigma(x,\phi))$ 的參數則是由編碼器訓練出來的，有 $z=e^\sigma\epsilon+\mu$，其中 $\epsilon\sim N(0,I)$，至此，我們得到任一樣本在潛空間的表現形式，可以想像成一種降維的方法，或者另一種編碼形式。假設我們輸入是一張 $512'512$ 的影像，如果分配 $q$ 的輸出維度為 $100$，那我們只需要 $100$ 維的向量就足夠表現原來這張圖片

### VAE 的極限

在一般形式的 VAE 中，所使用的 $q$ 分配通常為高斯分配，所以不管你的神經網路再怎麼厲害，潛變量 $z$ 仍然會服從高斯分配，這樣對厚尾或者有偏斜的資料很不友善，SIVI 的解決方案是新增更多的隨機層 ($M$)，並在每一層的輸入都加上 $\epsilon_t$

![](https://i.imgur.com/OJ9xYMc.jpg)

你可能會好奇，SIVI $q_{\phi}(z\midx)$ 最後輸出不還是一個高斯分配嗎? 怎麼就特別了? 其實你注意到，當我們訓練完模型之後，模型的參數是已經固定了，給定一個樣本 $x_i$，$q_{\phi}(z\midx)$ 的參數都已經是一個確定的值了，雖然在普通 VAE 會在潛變量上加上一個 $\epsilon$ 確保結果在一定範圍內變化是一致的，但是仍舊是單一的高斯分配，SIVI 不一定，因為在每一層 Layer 都有加入一個隨機量，所以最後生成出來的結果是一個隨機向量，並不是一個定值，這樣就會讓你的分配產生不同的高斯分配
